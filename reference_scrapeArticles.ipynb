{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlwqKTaoKglZ"
   },
   "source": [
    "# scrapeArticles\n",
    "A notebook for scraping article data from the ProQuest Newspapers Archive via scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Wjwh4a3Kgld"
   },
   "source": [
    "### Parameters, Dependencies, and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZFX9F7WKglg"
   },
   "outputs": [],
   "source": [
    "event_type = 'terroristattack' #@param {type:\"string\"}\n",
    "scrape_window = 50 #@param {type:\"integer\"}\n",
    "homepath = 'C:/Users/me/Google Drive/newscycle' #@param {type:\"string\"}\n",
    "\n",
    "# for math\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# for collecting and storing data\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import scrapy\n",
    "import itertools\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "# for troubleshooting\n",
    "import logging\n",
    "from scrapy.utils.response import open_in_browser\n",
    "\n",
    "# download/organize already existing dataset so we can avoid redundant scraping\n",
    "event_label = event_type\n",
    "event_type = event_type.replace(' ', '').lower()\n",
    "try:\n",
    "    articles = []\n",
    "    with open(os.path.join(homepath, event_type, 'data/articles.jsonl')) as f:\n",
    "        for line in f:\n",
    "            articles.append(json.loads(line))\n",
    "    articles = sorted(articles, key = lambda element: (int(element['databaseindex']), int(element['searchindex'])))\n",
    "    databaseindices = np.array([a['databaseindex'] for a in articles])\n",
    "    articles = np.array(articles)\n",
    "except FileNotFoundError:\n",
    "    articles = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbixfJkAKgly"
   },
   "source": [
    "### Search Parameter Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J15dbZkRKgl0"
   },
   "source": [
    "#### Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fek6hgR7Kgl2"
   },
   "outputs": [],
   "source": [
    "## super bowls\n",
    "def superbowlSearchGenerator(line, header, d0=None, d1=None):\n",
    "    # grab date\n",
    "    if not d0:\n",
    "        datestring = line[header.index('Date')]\n",
    "        d0 = datetime.datetime.strptime(datestring, '%b %d %Y')\n",
    "    if not d1:    \n",
    "        d1 = d0 + timedelta(days=scrape_window)\n",
    "\n",
    "    # query is just date and \"superbowl\"\n",
    "    query = 'PD({}-{}) AND (\"superbowl\" OR \"super bowl\")'.format(d0.strftime('%Y%m%d'), d1.strftime('%Y%m%d'))\n",
    "    return query, d0, d1\n",
    "\n",
    "def sotuSearchGenerator(line, header, d0=None, d1=None):\n",
    "    # grab date\n",
    "    if not d0:\n",
    "        datestring = line[header.index('date')]\n",
    "        d0 = parser.parse(datestring)\n",
    "    if not d1:    \n",
    "        d1 = d0 + timedelta(days=scrape_window)\n",
    "\n",
    "    # query is just date and \"superbowl\"\n",
    "    query = 'PD({}-{}) AND (\"state of the union\")'.format(d0.strftime('%Y%m%d'), d1.strftime('%Y%m%d'))\n",
    "    return query, d0, d1\n",
    "\n",
    "def worldseriesSearchGenerator(line, header, d0=None, d1=None):\n",
    "    # grab date\n",
    "    if not d0:\n",
    "        datestring = line[header.index('date')]\n",
    "        d0 = parser.parse(datestring)\n",
    "    if not d1:    \n",
    "        d1 = d0 + timedelta(days=scrape_window)\n",
    "\n",
    "    # query is just date and \"superbowl\"\n",
    "    query = 'PD({}-{}) AND (\"world series\")'.format(d0.strftime('%Y%m%d'), d1.strftime('%Y%m%d'))\n",
    "    return query, d0, d1\n",
    "\n",
    "def oscarSearchGenerator(line, header, d0=None, d1=None):\n",
    "    # grab date\n",
    "    if not d0:\n",
    "        datestring = line[header.index('date')]\n",
    "        d0 = parser.parse(datestring)\n",
    "    if not d1:    \n",
    "        d1 = d0 + timedelta(days=scrape_window)\n",
    "\n",
    "    # query is just date and \"oscar\"\n",
    "    query = 'PD({}-{}) AND (\"oscars\" OR \"academy awards\")'.format(d0.strftime('%Y%m%d'), d1.strftime('%Y%m%d'))\n",
    "    return query, d0, d1\n",
    "\n",
    "## terrorist attacks\n",
    "proximityparam = 200    # required proximacy of query terms to one another\n",
    "x = 2                   # number of keywords/phrases to require in a search result\n",
    "def terroristattackSearchGenerator(line, header, d0=None, d1=None):\n",
    "    \n",
    "    # location and date\n",
    "    location = ('(\"' + line[header.index('city')] + '\" OR \"' + line[header.index('provstate')] + '\")')\n",
    "    \n",
    "    if not d0:\n",
    "        if line[header.index('iday')] != str(0):\n",
    "            d0 = datetime.datetime(int(line[header.index('iyear')]),\n",
    "                                   int(line[header.index('imonth')]),\n",
    "                                   int(line[header.index('iday')]))\n",
    "        else:\n",
    "            d0 = datetime.datetime(int(line[header.index('iyear')]),\n",
    "                                   int(line[header.index('imonth')]), 1) #woah this is very wrong\n",
    "    if not d1:\n",
    "        d1 = d0 + timedelta(days=scrape_window)\n",
    "\n",
    "    # (City, State) AND ((shooting) or (bombing) or (bomb) or (violence) or (murder) or (terrorism))\n",
    "    #query = ('FT(' + location + ') AND (FT(shooting) OR FT(bombing) OR ' +\n",
    "    #         'FT(bomb) OR FT(violence) OR FT(murder) OR FT(terrorism)) ' +\n",
    "    #         'AND PD(' + d0.strftime('%Y%m%d') + '-' + d1.strftime('%Y%m%d') + ')')\n",
    "    query = 'PD({}-{}) AND {}'.format(\n",
    "        d0.strftime('%Y%m%d'),  d1.strftime('%Y%m%d'),\n",
    "        xof(x,[\n",
    "                 attackkeywords(line[header.index('attacktype1')]),\n",
    "                 targetkeywords(line[header.index('targtype1')],\n",
    "                                line[header.index('targsubtype1_txt')],\n",
    "                                line[header.index('corp1')],\n",
    "                                line[header.index('target1')]),\n",
    "                 perpkeywords(line[header.index('gname')]),\n",
    "                 #weaponkeywords(line[header.index('suicide')],\n",
    "                 #               line[header.index('attacktype1')],\n",
    "                 #               line[header.index('weaptype1')],\n",
    "                 #               line[header.index('weapsubtype1')]),\n",
    "                 #misckeywords(line[header.index('attacktype1')],\n",
    "                 #             line[header.index('ishostkid')],\n",
    "                 #             line[header.index('ransom')],\n",
    "                 #             line[header.index('suicide')]),\n",
    "                 'terroris*'\n",
    "             ], location))\n",
    "    return query, d0, d1\n",
    "\n",
    "searchParamGenerators = {'terroristattack': terroristattackSearchGenerator, 'superbowl': superbowlSearchGenerator, 'sotu': sotuSearchGenerator,\n",
    "                        'worldseries': worldseriesSearchGenerator, 'oscar': oscarSearchGenerator} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vh0_jmgkKgmH"
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_dxGCztKgmJ"
   },
   "outputs": [],
   "source": [
    "def xof(x, options):\n",
    "    # filters blank options and wraps them in parentheses\n",
    "    options = ['(' + each + ')' for each in options if len(each) > 0]\n",
    "    \n",
    "    # sets x to minimum of number of options and specified maximum limit\n",
    "    x = min(len(options), x)\n",
    "    \n",
    "    # \n",
    "    result = [' NEAR/{} '.format(proximityparam).join(list(combination))\n",
    "              for combination in itertools.combinations(options, x)]\n",
    "    return '((' + ') OR ('.join(result) + '))'\n",
    "\n",
    "def xof(x, options, location):\n",
    "    # filters blank options and wraps them in parentheses\n",
    "    options = ['(' + each + ')' for each in options if len(each) > 0]\n",
    "    \n",
    "    # sets x to minimum of number of options and specified maximum limit\n",
    "    x = min(len(options), x)\n",
    "    \n",
    "    # builds set of possible ways to fulfill constraints\n",
    "    result = []\n",
    "    for xcombo in itertools.combinations(options, x):\n",
    "        constraints = ['{1} NEAR/{0} {2}'.format(proximityparam, *list(nearcombo))\n",
    "                       for nearcombo in itertools.combinations(list(xcombo) + [location], 2)]\n",
    "        result.append(' AND '.join(constraints))\n",
    "    \n",
    "    return '((' + ') OR ('.join(result) + '))'\n",
    "    \n",
    "# define set of keywords such that the presence of one \n",
    "def attackkeywords(attacktype1):\n",
    "    keywords = []\n",
    "    \n",
    "    # parse attacktype1 - the attack category\n",
    "    if attacktype1 == '1':\n",
    "        keywords.append('assassin*')\n",
    "    elif attacktype1 == '2':\n",
    "        keywords.append('assault*')\n",
    "        keywords.append('armed')\n",
    "        pass\n",
    "    elif attacktype1 == '3':\n",
    "        keywords.append('bomb*')\n",
    "        keywords.append('explo*')\n",
    "    elif attacktype1 == '4':\n",
    "        keywords.append('hijack*')\n",
    "    elif attacktype1 == '5':\n",
    "        keywords.append('hostage')\n",
    "        keywords.append('barricade*')\n",
    "    elif attacktype1 == '6':\n",
    "        keywords.append('hostage*')\n",
    "        keywords.append('kidnap*')\n",
    "    elif attacktype1 == '7':\n",
    "        keywords.append('facility')\n",
    "        keywords.append('infrastructure')\n",
    "        keywords.append('sabotage')\n",
    "    elif attacktype1 == '8':\n",
    "        keywords.append('assault*')\n",
    "        keywords.append('unarmed')\n",
    "        \n",
    "    if len(keywords) > 0:\n",
    "        return '(' + ') OR ('.join(keywords) + ')'\n",
    "    else:\n",
    "        return ''\n",
    "        \n",
    "def targetkeywords(targtype1, targsubtype1_txt, corp1, target1):\n",
    "    keywords = []\n",
    "    \n",
    "    if targtype1 == '1':\n",
    "        keywords.append('business')\n",
    "    elif targtype1 == '2' or targtype1 == '22':\n",
    "        keywords.append('government')\n",
    "        keywords.append('political')\n",
    "    elif targtype1 == '3':\n",
    "        keywords.append('police')\n",
    "    elif targtype1 == '4':\n",
    "        keywords.append('military')\n",
    "    elif targtype1 == '5':\n",
    "        keywords.append('abortion')\n",
    "    elif targtype1 == '6':\n",
    "        keywords.append('airport')\n",
    "        keywords.append('aircraft')\n",
    "    elif targtype1 == '7':\n",
    "        keywords.append('government')\n",
    "        keywords.append('embass*')\n",
    "        keywords.append('consul*')\n",
    "    elif targtype1 == '8':\n",
    "        keywords.append('school')\n",
    "        keywords.append('\"educational institution\"')\n",
    "        keywords.append('university')\n",
    "        keywords.append('teach*')\n",
    "        keywords.append('professor')\n",
    "    elif targtype1 == '9':\n",
    "        keywords.append('supplies')\n",
    "    elif targtype1 == '10':\n",
    "        keywords.append('journalist')\n",
    "        keywords.append('reporter')\n",
    "        keywords.append('media')\n",
    "    elif targtype1 == '11':\n",
    "        keywords.append('maritime')\n",
    "        keywords.append('fishing')\n",
    "        keywords.append('\"oil tanker\"')\n",
    "        keywords.append('ferr*')\n",
    "        keywords.append('yacht')\n",
    "    elif targtype1 == '12':\n",
    "        keywords.append('NGO')\n",
    "        keywords.append('\"non-governmental organization\"')\n",
    "    elif targtype1 == '15':\n",
    "        keywords.append('religious')\n",
    "        keywords.append('church')\n",
    "        keywords.append('mosque')\n",
    "        keywords.append('synagogue')\n",
    "        keywords.append('imam')\n",
    "        keywords.append('priest')\n",
    "        keywords.append('bishop')\n",
    "    elif targtype1 == '16':\n",
    "        keywords.append('telecom*')\n",
    "        keywords.append('transmitter')\n",
    "        keywords.append('tower')\n",
    "    elif targtype1 == '18':\n",
    "        keywords.append('tourist')\n",
    "        keywords.append('\"tour bus*\"')\n",
    "        keywords.append('tour')\n",
    "    elif targtype1 == '19':\n",
    "        keywords.append('\"public transport*\"')\n",
    "    elif targtype1 == '21':\n",
    "        keywords.append('utilit*')\n",
    "        keywords.append('\"power line\"')\n",
    "        keywords.append('pipeline')\n",
    "        keywords.append('transformer')\n",
    "        keywords.append('\"high tension line\"')\n",
    "        keywords.append('substation')\n",
    "        keywords.append('lamppost')\n",
    "        keywords.append('\"street light\"')\n",
    "\n",
    "    targsubtype1_txt.replace('/Other Personnel', '')\n",
    "    targsubtype1_txt.replace('/Facility', '')\n",
    "    targsubtype1_txt.replace('/Ethnicity Identified', '')\n",
    "    targsubtype1_txt.replace('Religion Identified', 'Religious')\n",
    "    if targsubtype1_txt == 'Labor Union Related':\n",
    "        targsubtype1_txt = 'Labor Union/Union'\n",
    "    if targsubtype1_txt == 'Affiliated Institution':\n",
    "        targsubtype1_txt = ''\n",
    "    if targsubtype1_txt == 'Named Citizen':\n",
    "        targsubtype1_txt = ''\n",
    "    if targsubtype1_txt == 'Other (including online news agencies)':\n",
    "        targsubtype1_txt = ''\n",
    "    if targsubtype1_txt == 'Other Personnel':\n",
    "        targsubtype1_txt = ''\n",
    "    if targsubtype1_txt == 'Clinics':\n",
    "        targsubtype1_txt = 'Abortion Clinics'\n",
    "    if targsubtype1_txt == 'Personnel':\n",
    "        targsubtype1_txt = 'Abortion Personnel'\n",
    "    if targsubtype1_txt.count('(') > 0 or targsubtype1_txt.count(')') > 0:\n",
    "        regex = re.compile(\".*?\\((.*?)\\)\")\n",
    "        result = re.findall(regex, targsubtype1_txt)\n",
    "        targsubtype1_txt = targsubtype1_txt[:targsubtype1_txt.find('(' + result[0] + ')')-1]\n",
    "    \n",
    "    targsubtype1_txt = ['\"' + each.strip().rstrip() + '\"' for each in targsubtype1_txt.split('/')]\n",
    "    keywords += targsubtype1_txt\n",
    "    \n",
    "    if len(corp1) > 0:\n",
    "        keywords.append('\"' + corp1 + '\"')\n",
    "    \n",
    "    if len(target1) > 0:\n",
    "        keywords.append('\"' + target1 + '\"')\n",
    "    \n",
    "    keywords = [k for k in keywords if len(k) > 1]\n",
    "    if len(keywords) > 0:\n",
    "        return '(' + ') OR ('.join(keywords) + ')'\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "def perpkeywords(gname):\n",
    "    if len(gname) > 2 and gname != 'Unknown':\n",
    "        return '(\"' + gname + '\")'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def weaponkeywords(suicide, attacktype1, weaptype1, weaptype2):\n",
    "    keywords = []\n",
    "    \n",
    "    if weaptype1 == '1':\n",
    "        keywords.append('biological')\n",
    "    elif weaptype1 == '2':\n",
    "        keywords.append('chemical')\n",
    "    elif weaptype1 == '3':\n",
    "        keywords.append('radiological')\n",
    "        keywords.append('radioactive')\n",
    "        keywords.append('radiation')\n",
    "    elif weaptype1 == '4':\n",
    "        keywords.append('nuclear')\n",
    "    elif weaptype1 == '5':\n",
    "        keywords.append('firearm')\n",
    "        keywords.append('gun')\n",
    "    elif weaptype1 == '6' and attacktype1 != '3':\n",
    "        keywords.append('bomb*')\n",
    "        keywords.append('explo*')\n",
    "    elif weaptype1 == '7':\n",
    "        keywords.append('fake')\n",
    "    elif weaptype1 == '8':\n",
    "        keywords.append('incendiary')\n",
    "        keywords.append('arson')\n",
    "        keywords.append('combustible')\n",
    "        keywords.append('flammable')\n",
    "        keywords.append('inflammable')\n",
    "        keywords.append('fire')\n",
    "    elif weaptype1 == '9':\n",
    "        keywords.append('melee')\n",
    "    elif weaptype1 == '10':\n",
    "        keywords.append('vehicle')\n",
    "        keywords.append('car')\n",
    "        keywords.append('bus')\n",
    "        keywords.append('truck')\n",
    "        keywords.append('van')\n",
    "        keywords.append('automobile')\n",
    "    elif weaptype1 == '11' and attacktype1 != '7':\n",
    "        keywords.append('sabotage')\n",
    "    \n",
    "    if weaptype2 == '1':\n",
    "        keywords.append('poison*')\n",
    "    elif weaptype2 == '30':\n",
    "        keywords.append('explo*')\n",
    "    elif weaptype2 == '2':\n",
    "        keywords.append('automatic')\n",
    "        keywords.append('semi-automatic')\n",
    "    elif weaptype2 == '3':\n",
    "        keywords.append('handgun')\n",
    "    elif weaptype2 == '4':\n",
    "        keywords.append('rifle')\n",
    "        keywords.append('shotgun')\n",
    "    elif (weaptype2 == '5' or weaptype2 == '6') and weaptype1 != '5':\n",
    "        keywords.append('firearm')\n",
    "        keywords.append('gun')\n",
    "    elif weaptype2 == '7':\n",
    "        keywords.append('grenade')\n",
    "    elif weaptype2 == '8':\n",
    "        keywords.append('mine')\n",
    "    elif weaptype2 == '9':\n",
    "        for keyword in ['\"parcel bomb\"', '\"mail bomb\"', '\"package bomb\"', '\"note bomb\"', '\"message bomb\"',\n",
    "                        '\"gift bomb\"', '\"present bomb\"','\"delivery bomb\"', '\"surprise bomb\"', '\"postal bomb\"',\n",
    "                        '\"post bomb\"']:\n",
    "            keywords.append(keyword)\n",
    "    elif weaptype2 == '10':\n",
    "        keywords.append('\"pressure trigger\"')\n",
    "    elif weaptype2 == '11':\n",
    "        for keyword in ['projectile', 'rocket', 'mortar', 'RPG', 'missile']:\n",
    "            keywords.append(keyword)\n",
    "    elif weaptype2 == '12':\n",
    "        for keyword in ['\"remote device\"', 'trigger', 'detonate']:\n",
    "            keywords.append(keyword)\n",
    "    elif weaptype2 == '13' and suicide != '1':\n",
    "        keywords.append('suicide')\n",
    "    elif weaptype2 == '14':\n",
    "        keywords.append('\"time fuse\"')\n",
    "    elif weaptype2 == '15' and weaptype1 != '10':\n",
    "        keywords.append('vehicle')\n",
    "        keywords.append('car')\n",
    "        keywords.append('bus')\n",
    "        keywords.append('truck')\n",
    "        keywords.append('van')\n",
    "        keywords.append('automobile')\n",
    "    elif (weaptype2 == '16' or weaptype2 == '17') and weaptype1 != '6' and attacktype1 != '3':\n",
    "        keywords.append('bomb*')\n",
    "        keywords.append('explo*')\n",
    "    elif weaptype2 == '28':\n",
    "        keywords.append('dynamite')\n",
    "        keywords.append('tnt')\n",
    "    elif weaptype2 == '29':\n",
    "        keywords.append('\"sticky bomb\"')\n",
    "    elif weaptype2 == '18' and weaptype1 != '8':\n",
    "        keywords.append('incendiary')\n",
    "        keywords.append('arson')\n",
    "        keywords.append('combustible')\n",
    "        keywords.append('flammable')\n",
    "        keywords.append('inflammable')\n",
    "        keywords.append('fire')\n",
    "    elif weaptype2 == '19':\n",
    "        keywords.append('molotov')\n",
    "        keywords.append('\"petrol bomb\"')\n",
    "    elif weaptype2 == '20':\n",
    "        keywords.append('gasoline')\n",
    "        keywords.append('alcohol')\n",
    "    elif weaptype2 == '21':\n",
    "        keywords.append('blunt')\n",
    "    elif weaptype2 == '22':\n",
    "        keywords.append('fist')\n",
    "        keywords.append('punch*')\n",
    "        keywords.append('beat*')\n",
    "        keywords.append('kick*')\n",
    "    elif weaptype2 == '23':\n",
    "        keywords.append('knife')\n",
    "        keywords.append('sword')\n",
    "        keywords.append('stab')\n",
    "    elif weaptype2 == '24':\n",
    "        keywords.append('rope')\n",
    "        keywords.append('strangl*')\n",
    "    elif weaptype2 == '26':\n",
    "        keywords.append('suffocat*')\n",
    "    \n",
    "    keywords = [k for k in keywords if len(k) > 1]\n",
    "    if len(keywords) > 0:\n",
    "        return '(' + ') OR ('.join(keywords) + ')'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def misckeywords(attacktype1, ishostkid, ransom, suicide):\n",
    "    keywords = []\n",
    "    \n",
    "    if suicide == '1':\n",
    "        keywords.append('suicide')\n",
    "    if ransom == '1':\n",
    "        keywords.append('ransom')\n",
    "    if ishostkid == '1' and attacktype1 != '5' and attacktype1 != '6':\n",
    "        keywords.append('hostage*')\n",
    "        keywords.append('kidnap*')\n",
    "    \n",
    "    keywords = [k for k in keywords if len(k) > 1]\n",
    "    if len(keywords) > 0:\n",
    "        return '(' + ') OR ('.join(keywords) + ')'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Dh1qjk-KgmW"
   },
   "outputs": [],
   "source": [
    "def parseDate(info, eventdate, maxdays):\n",
    "    # remove the duplicate label\n",
    "    original = info\n",
    "    info = info.replace(' [Duplicate]', '')\n",
    "    \n",
    "    # remove content after a second-to-last comma if two exist\n",
    "    try:\n",
    "        info = ', '.join(info.split(', ')[-2:])\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # remove everything before a first parenthesis if one exists\n",
    "    try:\n",
    "        info = info[info.rfind('(')+1:]\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # between periods\n",
    "    try:\n",
    "        datetext = info.split('. ')[-2]\n",
    "        parsed = parser.parse(datetext, fuzzy=True)\n",
    "        assert 0 <= (parsed-eventdate).days < maxdays\n",
    "        return parser.parse(datetext, fuzzy=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # before last bracket, after last colon\n",
    "    try:\n",
    "        datetext = info[info.rfind(']')+1:]\n",
    "        datetext = datetext[:datetext.rfind(':')]\n",
    "        datetext = datetext.replace(')', '')\n",
    "        assert any([str(i) in datetext for i in range(10)])\n",
    "        parsed = parser.parse(datetext, fuzzy=True)\n",
    "        assert 0 <= (parsed-eventdate).days < maxdays\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # remove everything before last bracket\n",
    "    try:\n",
    "        datetext = info[info.rfind(']')+1:]\n",
    "        parsed = parser.parse(datetext, fuzzy=True)\n",
    "        assert 0 <= (parsed-eventdate).days < maxdays\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # remove everything after last colon\n",
    "    try:\n",
    "        datetext = info[:info.rfind(':')]\n",
    "        parsed = parser.parse(datetext, fuzzy=True)\n",
    "        assert 0 <= (parsed-eventdate).days < maxdays\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # see if parser can solve by itself\n",
    "    try:\n",
    "        datetext = info\n",
    "        parsed = parser.parse(datetext, fuzzy=True)\n",
    "        assert 0 <= (parsed-eventdate).days < maxdays\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # try again without content after first colon\n",
    "    if ':' in original:\n",
    "        return parseDate(original[:original.rfind(':')], eventdate, maxdays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPpjDhYfKgmi"
   },
   "source": [
    "### Scraping Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNSB4enVKgmk"
   },
   "outputs": [],
   "source": [
    "# organizes the information we scrape about each article\n",
    "class ArticleItem(scrapy.Item):\n",
    "    \n",
    "    # info defined by event data set\n",
    "    databaseindex = scrapy.Field()\n",
    "    \n",
    "    # info defined by search process\n",
    "    resultscount = scrapy.Field()\n",
    "    query = scrapy.Field()\n",
    "    originalquery = scrapy.Field()\n",
    "    originalstart = scrapy.Field()\n",
    "    originalend = scrapy.Field()\n",
    "    querystart = scrapy.Field()\n",
    "    queryend = scrapy.Field()\n",
    "    parents = scrapy.Field()\n",
    "    \n",
    "    # info defined by article content\n",
    "    searchindex = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    info = scrapy.Field()\n",
    "    link  = scrapy.Field()\n",
    "    \n",
    "    # info derived from those above\n",
    "    daysFrom = scrapy.Field()\n",
    "\n",
    "# what happens to each generated article item,\n",
    "# stores all scraped items into a single articles.jl file\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # operations performed when spider starts\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open(os.path.join(homepath, event_type, 'data/articles.jsonl'), 'a')\n",
    "\n",
    "    # when the spider finishes\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # when the spider yields an item\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YkbFZd0Kgmw"
   },
   "source": [
    "### Spider Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qnIS3ahKgmy"
   },
   "outputs": [],
   "source": [
    "header = None\n",
    "maxpossiblepages = 100\n",
    "\n",
    "class articleSpider(scrapy.Spider):\n",
    "    name = 'superbowl'\n",
    "    custom_settings = {'HTTPERROR_ALLOWED_CODES': [500],\n",
    "                      'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1},\n",
    "                      'LOG_LEVEL': 'WARNING'}\n",
    "    \n",
    "    def start_requests(self):\n",
    "        global header\n",
    "        totalsearches = 0\n",
    "        \n",
    "        # scan through database\n",
    "        counter = 0\n",
    "        f = open(os.path.join(homepath, event_type, 'data/{}s.csv'.format(event_type)), encoding='utf-8')\n",
    "        event_csv = csv.reader(f)\n",
    "        t = tqdm(total=1630)\n",
    "        \n",
    "        for line in event_csv:\n",
    "            counter += 1\n",
    "            t.update()\n",
    "            \n",
    "            if counter == 1:\n",
    "                header = line\n",
    "                continue\n",
    "                \n",
    "            # generate search content\n",
    "            query, d0, d1 = searchParamGenerators[event_type](line, header)\n",
    "            \n",
    "            # skip line if query is empty\n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            # if no results exist at all in existing data set, search is a-go as before;\n",
    "            # otherwise constrain search to avoid redundancy\n",
    "            if articles is not None:\n",
    "                if np.size(articles[databaseindices==counter]) == 0:\n",
    "                    missing = 'All'\n",
    "                else:\n",
    "                    count = min([int(a['resultscount']) for a in articles[databaseindices==counter] if a['parents'] == 0])\n",
    "                    missing = set(np.arange(1, count+1)) - set([int(s['searchindex']) for s in articles[databaseindices==counter]])\n",
    "            else:\n",
    "                missing = 'All'\n",
    "            \n",
    "            # don't do any search if no results are missing for this event\n",
    "            if not missing:\n",
    "                continue\n",
    "    \n",
    "            totalsearches += 1\n",
    "            logging.warning(counter)\n",
    "            logging.warning(missing)\n",
    "            yield scrapy.Request('https://search.proquest.com/advanced.showresultpageoptions?site=news',\n",
    "                                     callback=self.startform, dont_filter=True, \n",
    "                                     meta={'originalquery': query, 'query': query, 'databaseindex': counter,\n",
    "                                           'originalstart': d0, 'originalend': d1, 'line': line, \n",
    "                                           'querystart': d0, 'queryend': d1, 'parents': 0, 'missing': missing}\n",
    "                                )\n",
    "        tqdm.close()\n",
    "            \n",
    "     # starts the form that must be filled out to search w/ our query\n",
    "    def startform(self, response):\n",
    "        # start the search form\n",
    "        yield scrapy.Request('https://search.proquest.com/news/advanced?accountid=13314',\n",
    "                             callback=self.query, dont_filter=True, meta=response.meta)\n",
    "    \n",
    "    # fills out form and initiates search\n",
    "    def query(self, response):\n",
    "        # fill it out and search\n",
    "        yield scrapy.FormRequest.from_response(response, dont_filter=True, formid='searchForm',\n",
    "                                               formdata={'queryTermField': response.meta['query'],'fullTextLimit':'on',\n",
    "                                                         'sortType':'DateAsc', 'includeDuplicate':'on'},\n",
    "                                               callback=self.parsePages, clickdata={'id': 'searchToResultPage'},\n",
    "                                               meta=response.meta)\n",
    "    \n",
    "    # sets up inspection of each page of results generated by search\n",
    "    def parsePages(self, response):\n",
    "\n",
    "        sel = Selector(response)\n",
    "        if 'sessionexpired' in response.url:\n",
    "            logging.warning('Session Expiration Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "            return\n",
    "        try:\n",
    "            resultscount = sel.xpath(\"//h1[@id='pqResultsCount']/text()\").extract()[0]\n",
    "        except IndexError:\n",
    "            logging.warning('Result Absence Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "            return\n",
    "        resultscount = int(resultscount[:resultscount.find(' ')].replace(',', ''))\n",
    "        maxpages = resultscount // 100\n",
    "        urlparts = [response.url[:response.url.find('/1')+1], response.url[response.url.find('1?')+1:]]\n",
    "\n",
    "        # what i do next depends on what's missing\n",
    "        # for each result page, grab and parse it if a needed result is missing\n",
    "        # if there's a missing result beyond the max possible recount, open the final result page at the end of the loop\n",
    "        for page_index in range(min(maxpages+1, maxpossiblepages)):\n",
    "            request = scrapy.Request(str(page_index+1).join(urlparts), callback=self.parse, dont_filter=True, meta=response.meta)\n",
    "\n",
    "            if response.meta['missing'] is 'All':\n",
    "                yield request\n",
    "            elif 0 < len(set(np.arange((page_index*100)+1+(response.meta['parents']*maxpossiblepages*100),min((page_index+1)*100+(response.meta['parents']*maxpossiblepages*100),\n",
    "                                                                                     resultscount+(response.meta['parents']*maxpossiblepages*100))+1)\n",
    "                   ).intersection(response.meta['missing'])):\n",
    "                yield request\n",
    "            elif page_index+1 == maxpossiblepages and len([m for m in response.meta['missing'] if m > maxpossiblepages*100]) > 0:\n",
    "                yield request\n",
    "            \n",
    "    def parse(self, response):\n",
    "        sel = Selector(response)\n",
    "        if 'sessionexpired' in response.url:\n",
    "            logging.warning('Session Expiration Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "            return\n",
    "        try:\n",
    "            resultscount = sel.xpath(\"//h1[@id='pqResultsCount']/text()\").extract()[0]\n",
    "        except IndexError:\n",
    "            logging.warning('Result Absence Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "            return\n",
    "        resultscount = int(resultscount[:resultscount.find(' ')].replace(',', ''))\n",
    "        \n",
    "        indices = sel.xpath(\"//li[@class='resultItem ltr']/div//span[@class='indexing']/text()\").extract()\n",
    "        titles = sel.xpath(\"//h3/a/@title\").extract()\n",
    "        links = sel.xpath(\"//h3/a/@href\").extract()\n",
    "        info = [(' '.join(path.xpath(\".//span[@class='titleAuthorETC']//text()\").extract())).replace('\\n', '') for path in sel.xpath(\"//li[@class='resultItem ltr']\")]\n",
    "\n",
    "        # try to extract date from each article's info\n",
    "        \n",
    "        dates = [parseDate(each, response.meta['originalstart'], scrape_window+1) for each in info]\n",
    "        \n",
    "        # correct me if im wrong but i assume all of these lists are of the same length\n",
    "        assert (len(indices) + len(titles) + len(links) + len(info) + len(dates)) == (len(indices) + len(indices) + len(indices) + len(indices) + len(indices))\n",
    "        \n",
    "        # now populate an ArticleItem() for each result\n",
    "        for i in range(len(indices)):\n",
    "            \n",
    "            # but skip if missing parameter suggests that the articleitem has already been processed\n",
    "            if response.meta['missing'] is not 'All':\n",
    "                if int(indices[i]) + response.meta['parents']*maxpossiblepages*100 not in response.meta['missing']:\n",
    "                    continue\n",
    "            \n",
    "            article = ArticleItem()\n",
    "            \n",
    "            # defined by database\n",
    "            article['databaseindex'] = response.meta['databaseindex']\n",
    "            \n",
    "            # defined prior to or at start of search\n",
    "            article['resultscount'] = resultscount + response.meta['parents']*maxpossiblepages*100\n",
    "            article['originalquery'] = response.meta['originalquery']\n",
    "            article['originalstart'] = str(response.meta['originalstart'])\n",
    "            article['originalend'] = str(response.meta['originalend'])\n",
    "            article['query'] = response.meta['query']\n",
    "            article['querystart'] = str(response.meta['querystart'])\n",
    "            article['queryend'] = str(response.meta['queryend'])\n",
    "            article['parents'] = int(response.meta['parents'])\n",
    "\n",
    "            # defined by item itself\n",
    "            article['searchindex'] = int(indices[i]) + response.meta['parents']*maxpossiblepages*100\n",
    "            article['title'] = titles[i]\n",
    "            article['info'] = info[i]\n",
    "            article['link']  = links[i]\n",
    "\n",
    "            # derived from those above\n",
    "            article['daysFrom'] = str(dates[i])\n",
    "            yield article\n",
    "            \n",
    "        # set up successive searches for when there are more than max possible results\n",
    "        limitstring = 'You have reached the maximum number of search results that are displayed.'\n",
    "        limit = sel.xpath(\"//p[@class='errorMessageHeaderText']/text()\")\n",
    "        if limit:\n",
    "            if limitstring in limit.extract()[0]:\n",
    "                request = scrapy.Request('https://search.proquest.com/advanced.showresultpageoptions?site=news',\n",
    "                                         callback=self.startform, dont_filter=True, meta=response.meta)\n",
    "                \n",
    "                request.meta['parents'] += 1\n",
    "                request.meta['querystart'] = [d for d in dates if d is not None][-1]\n",
    "                request.meta['query'] = searchParamGenerators[event_type](request.meta['line'], header, d0=request.meta['querystart'], d1=request.meta['queryend'])[0]\n",
    "                yield request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TFCntK7Kgm-"
   },
   "source": [
    "### Spider Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1_ds8JiKgnA"
   },
   "outputs": [],
   "source": [
    "process = CrawlerProcess({'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})\n",
    "\n",
    "process.crawl(articleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC_S8iSqKgnQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "scrapeArticles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
