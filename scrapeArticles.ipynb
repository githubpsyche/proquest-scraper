{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlwqKTaoKglZ"
   },
   "source": [
    "# scrapeArticles\n",
    "A notebook for scraping news article data from the ProQuest Newspapers Archive via `scrapy`. A user with archive access should be able to specify search parameters and upon execution obtain an organized list of all relevant articles mentioned in the archive, including all metadata necessary to reproduce or locate any particular result.\n",
    "\n",
    "## Source Overview\n",
    "ProQUEST should be explained here (and what we're doing with it should probably be explained in more detail above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Wjwh4a3Kgld"
   },
   "source": [
    "## Dependencies\n",
    "Here we specify the libraries and basic data that our article scraping pipeline depends on to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for math\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# for scraping and storing data\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import scrapy\n",
    "import itertools\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "# for troubleshooting\n",
    "import logging\n",
    "from scrapy.utils.response import open_in_browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Space\n",
    "Here we define the space across ProQUEST for our news articles search. For now, we'll just assume only a single search `query` is a parameter varied from search to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify directory where data will be stored with a name for the current research topic\n",
    "topic = 'biden'\n",
    "\n",
    "# what will be searched\n",
    "query = 'biden' \n",
    "\n",
    "# must specify a date range (d0 to d1) so we can ensure search completeness later on. if not interested in constraining dates, just include every relevant date!\n",
    "# articles published on d0 up to but excluding d1 will be collected\n",
    "d0 = parser.parse('May 1, 2020')\n",
    "d1 = parser.parse('May 2, 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Pipeline\n",
    "Here we'll define our web crawler and its process for traversing and extracting the data we want from ProQUEST.\n",
    "\n",
    "### Minor Details\n",
    "\n",
    "#### We will load/organize already existing dataset so we can avoid redundant scraping\n",
    "We assume that the data will be located at `data/articles.jsonl` within a directory associated with the current research `topic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    articles = []\n",
    "    with open(os.path.join(topic, 'data', 'articles.jsonl')) as f:\n",
    "        for line in f:\n",
    "            articles.append(json.loads(line))\n",
    "    articles = np.array(articles)\n",
    "except FileNotFoundError:\n",
    "    articles = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll organize scraped information into an ArticleItem instance to facilitate orderly storage.\n",
    "There are two types of information we currently store: \n",
    "- **Information about the search process**. Every detail identifying we found this article using this pipeline so that anyone who wants to check our work (including ourselves) can do it.\n",
    "- **Information about the article**. Just meta-data for now rather than content. Stuff like title, publication, date, URL, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleItem(scrapy.Item):\n",
    "    \n",
    "    # info defined by search process\n",
    "    resultscount = scrapy.Field()\n",
    "    query = scrapy.Field()\n",
    "    originalquery = scrapy.Field()\n",
    "    originalstart = scrapy.Field()\n",
    "    originalend = scrapy.Field()\n",
    "    querystart = scrapy.Field()\n",
    "    queryend = scrapy.Field()\n",
    "    parents = scrapy.Field()\n",
    "    \n",
    "    # info defined by article content\n",
    "    searchindex = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    info = scrapy.Field()\n",
    "    link  = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll store Article Data as JSON lines.\n",
    "This `JsonWriterPipeline` class specifies exactly what happens when a new `ArticleItem` instance is prepared. We'll store all scraped items into a single `articles.jsonl`, listing each research as a unique JSON object.\n",
    "\n",
    "`JSON` is just a human-readable way of representing dictionaries as text. With the `json` package, they can be readily loaded into Python dictionaries or converted into other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # operations performed when spider starts\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open(os.path.join(topic, 'data', 'articles.jsonl'), 'a')\n",
    "\n",
    "    # when the spider finishes\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # when the spider yields an item\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Settings and Initial URL(s)\n",
    "The initial URL isn't actually the search form. Instead, we go to a URL that for some unknown reason must be visited first in order to have access to all possible search parameters with a web crawler. Query information is maintained in a `meta` field within the request so we use (and ultimately store) the information downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class articleSpider(scrapy.Spider):\n",
    "    name = 'articles'\n",
    "    custom_settings = {'HTTPERROR_ALLOWED_CODES': [500],\n",
    "                      'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1},\n",
    "                      'LOG_LEVEL': 'WARNING'}\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        # if no results exist at all in existing data set, search is a-go as before;\n",
    "        # otherwise constrain search to avoid redundancy\n",
    "        # this is a powerful way to test if and ensure our traversal actually succeeded\n",
    "        # since proquest will inevitably reject some request, some drop-outs are inevitable and must be tracked/corrected\n",
    "        if articles is not None:\n",
    "            if np.size(articles) == 0:\n",
    "                missing = 'All'\n",
    "            else:\n",
    "                count = min([int(a['resultscount']) for a in articles if a['parents'] == 0])\n",
    "                missing = set(np.arange(1, count+1)) - set([int(s['searchindex']) for s in articles])\n",
    "        else:\n",
    "            missing = 'All'\n",
    "        \n",
    "        yield scrapy.Request('https://search.proquest.com/advanced.showresultpageoptions?site=news',\n",
    "                                 callback=self.startform, dont_filter=True, \n",
    "                                 meta={'originalquery': query, 'query': query, 'databaseindex': 0,\n",
    "                                       'originalstart': d0, 'originalend': d1, 'line': '',\n",
    "                                       'querystart': d0, 'queryend': d1, 'parents': 0, 'missing': missing}\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying for Results\n",
    "We have to make a request to start the full search form and then another request to actually initiate the search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts the form that must be filled out to search w/ our query\n",
    "def startform(self, response):\n",
    "    # start the search form\n",
    "    yield scrapy.Request('https://search.proquest.com/news/advanced?accountid=13314',\n",
    "                         callback=self.query, dont_filter=True, meta=response.meta)\n",
    "\n",
    "# fills out form and initiates search\n",
    "def query(self, response):\n",
    "    # fill it out and search\n",
    "    yield scrapy.FormRequest.from_response(response, dont_filter=True, formid='searchForm',\n",
    "                                           formdata={'queryTermField': response.meta['query'],'fullTextLimit':'on',\n",
    "                                                     'sortType':'DateAsc', 'includeDuplicate':'on'},\n",
    "                                           callback=self.parsePages, clickdata={'id': 'searchToResultPage'},\n",
    "                                           meta=response.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning Traversal of Result Pages\n",
    "We generate a unique request for each page of the search results. Furthermore, since ProQUEST returns a maximum number of results associated with a particular search query that may be smaller than the number of *true* matching results, we may have to prepare to generate new searches excluding already returned results so that the missing results can be collected too.\n",
    "\n",
    "At the same time, we avoid querying for pages whose results are already stored in the relevant `data/articles.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up inspection of each page of results generated by search\n",
    "def parsePages(self, response):\n",
    "\n",
    "    sel = Selector(response)\n",
    "    \n",
    "    # sometimes proquest will expire the current session or refuse to fulfill a query\n",
    "    # we'll have to get them another time!\n",
    "    if 'sessionexpired' in response.url:\n",
    "        logging.warning('Session Expiration Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "        return\n",
    "    \n",
    "    # we check if there are no results provided for some other reason and also log/give up when that happens\n",
    "    try:\n",
    "        resultscount = sel.xpath(\"//h1[@id='pqResultsCount']/text()\").extract()[0]\n",
    "    except IndexError:\n",
    "        logging.warning('Result Absence Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "        return\n",
    "    \n",
    "    # on this page we can count the number of returned results and construct follow-up queries on that basis\n",
    "    resultscount = int(resultscount[:resultscount.find(' ')].replace(',', ''))\n",
    "    maxpages = resultscount // 100\n",
    "    urlparts = [response.url[:response.url.find('/1')+1], response.url[response.url.find('1?')+1:]]\n",
    "\n",
    "    # what i do next depends on what's missing\n",
    "    # for each result page, grab and parse it if a needed result is missing\n",
    "    # if there's a missing result beyond the max possible recount, open the final result page at the end of the loop\n",
    "    for page_index in range(min(maxpages+1, maxpossiblepages)):\n",
    "        request = scrapy.Request(str(page_index+1).join(urlparts), callback=self.parse, dont_filter=True, meta=response.meta)\n",
    "\n",
    "        if response.meta['missing'] is 'All':\n",
    "            yield request\n",
    "        elif 0 < len(set(np.arange((page_index*100)+1+(response.meta['parents']*maxpossiblepages*100),min((page_index+1)*100+(response.meta['parents']*maxpossiblepages*100),\n",
    "                                                                                 resultscount+(response.meta['parents']*maxpossiblepages*100))+1)\n",
    "               ).intersection(response.meta['missing'])):\n",
    "            yield request\n",
    "        elif page_index+1 == maxpossiblepages and len([m for m in response.meta['missing'] if m > maxpossiblepages*100]) > 0:\n",
    "            yield request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Results For Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "        sel = Selector(response)\n",
    "        \n",
    "        # sometimes proquest will expire the current session or refuse to fulfill a query\n",
    "        # we'll have to get them another time!\n",
    "        if 'sessionexpired' in response.url:\n",
    "            logging.warning('Session Expiration Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "            return\n",
    "        \n",
    "        # we check if there are no results provided for some other reason and also log/give up when that happens\n",
    "        try:\n",
    "            resultscount = sel.xpath(\"//h1[@id='pqResultsCount']/text()\").extract()[0]\n",
    "        except IndexError:\n",
    "            logging.warning('Result Absence Outcome Tied To {}'.format(response.meta['databaseindex']))\n",
    "            return\n",
    "        resultscount = int(resultscount[:resultscount.find(' ')].replace(',', ''))\n",
    "        \n",
    "        # we pull the data from the results page for parsing\n",
    "        indices = sel.xpath(\"//li[@class='resultItem ltr']/div//span[@class='indexing']/text()\").extract()\n",
    "        titles = sel.xpath(\"//h3/a/@title\").extract()\n",
    "        links = sel.xpath(\"//h3/a/@href\").extract()\n",
    "        info = [(' '.join(path.xpath(\".//span[@class='titleAuthorETC']//text()\").extract())).replace('\\n', '') for path in sel.xpath(\"//li[@class='resultItem ltr']\")]\n",
    "        \n",
    "        # correct me if im wrong but i assume all of these lists are of the same length\n",
    "        assert (len(indices) + len(titles) + len(links) + len(info) + len(dates)) == (len(indices) + len(indices) + len(indices) + len(indices) + len(indices))\n",
    "        \n",
    "        # now populate an ArticleItem() for each result\n",
    "        for i in range(len(indices)):\n",
    "            \n",
    "            # but skip if missing parameter suggests that the articleitem has already been processed\n",
    "            if response.meta['missing'] is not 'All':\n",
    "                if int(indices[i]) + response.meta['parents']*maxpossiblepages*100 not in response.meta['missing']:\n",
    "                    continue\n",
    "            \n",
    "            article = ArticleItem()\n",
    "            \n",
    "            # defined prior to or at start of search\n",
    "            article['resultscount'] = resultscount + response.meta['parents']*maxpossiblepages*100\n",
    "            article['originalquery'] = response.meta['originalquery']\n",
    "            article['originalstart'] = str(response.meta['originalstart'])\n",
    "            article['originalend'] = str(response.meta['originalend'])\n",
    "            article['query'] = response.meta['query']\n",
    "            article['querystart'] = str(response.meta['querystart'])\n",
    "            article['queryend'] = str(response.meta['queryend'])\n",
    "            article['parents'] = int(response.meta['parents'])\n",
    "\n",
    "            # defined by item itself\n",
    "            article['searchindex'] = int(indices[i]) + response.meta['parents']*maxpossiblepages*100\n",
    "            article['title'] = titles[i]\n",
    "            article['info'] = info[i]\n",
    "            article['link']  = links[i]\n",
    "\n",
    "            yield article\n",
    "            \n",
    "        # set up successive searches for when there are more than max possible results\n",
    "        limitstring = 'You have reached the maximum number of search results that are displayed.'\n",
    "        limit = sel.xpath(\"//p[@class='errorMessageHeaderText']/text()\")\n",
    "        if limit:\n",
    "            if limitstring in limit.extract()[0]:\n",
    "                request = scrapy.Request('https://search.proquest.com/advanced.showresultpageoptions?site=news',\n",
    "                                         callback=self.startform, dont_filter=True, meta=response.meta)\n",
    "                \n",
    "                request.meta['parents'] += 1\n",
    "                request.meta['querystart'] = [d for d in dates if d is not None][-1]\n",
    "                request.meta['query'] = searchParamGenerators[event_type](request.meta['line'], header, d0=request.meta['querystart'], d1=request.meta['queryend'])[0]\n",
    "                yield request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TFCntK7Kgm-"
   },
   "source": [
    "### Spider Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1_ds8JiKgnA"
   },
   "outputs": [],
   "source": [
    "articleSpider.startform = startform\n",
    "articleSpider.query = query\n",
    "articleSpider.parsePages = parsePages\n",
    "articleSpider.parse = parse\n",
    "\n",
    "process = CrawlerProcess({'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})\n",
    "\n",
    "process.crawl(articleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
